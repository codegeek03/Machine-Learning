{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af8d011",
   "metadata": {},
   "source": [
    "# Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b3f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28ce964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Loading pre-trained VGG19 architecture\n",
    "model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb32b4",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:720/format:webp/1*1ztswIVGxwwTYPsTEygSJg.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a705bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining content and style layers\n",
    "\n",
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2fde7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that outputs the content and style representations of the input image\n",
    "content_model = tf.keras.Model(inputs=model.input, outputs=[model.get_layer(layer).output for layer in content_layers])\n",
    "style_model = tf.keras.Model(inputs=model.input, outputs=[model.get_layer(layer).output for layer in style_layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff20f3",
   "metadata": {},
   "source": [
    "<h3>content_loss= ∑(Ccontent-Tcontent)²/2</h3>\n",
    "\n",
    "Ccontent = features extracted through the network of Original Content image.\n",
    "and Tcontent = features of the content of Target image.\n",
    "\n",
    "We will need to update out Tcontent such that the Content loss is minimum. We will be using the second convolutional layer in 4th stack of convolutional layer(conv4_2) to compare the content and target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7db328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(base,target):\n",
    "    return tf.reduce_mean(tf.square(base-target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ecc6f",
   "metadata": {},
   "source": [
    "<h4>Gram Matrix gives the correlation matrix between the features/filters.<br><br>\n",
    "    <img src='https://miro.medium.com/v2/resize:fit:828/format:webp/1*mbeEuJLyw_2atq3pi1PgAA.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4136614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    feature_maps=int(input_tensor.shape[-1])\n",
    "    a=tf.reshape(input_tensor.shape[-1])\n",
    "    n=tf.shape(a)[0]\n",
    "    gram=tf.matmul(a,a,transpose_a=True)\n",
    "    return gram/tf.cast(n,tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db576fb5",
   "metadata": {},
   "source": [
    "<h2>Style Loss:</h2>\n",
    "\n",
    "We want our target images style to be similar to our style image. So we define a metric to know how close is the style of target to the original style.\n",
    "\n",
    "We compare the gram matrix of the target and style image in every convolutional layer, to see how close is the style.\n",
    "\n",
    "<h3>Lstyle = ∑w(Cstyle-Tstyle)²</h3>\n",
    "\n",
    "Cstyle = gram matrix of original image style of each layer.<br>\n",
    "Tstyle = gram matrix of the target image.<br>\n",
    "w = style weight at each layer<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d410136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(base,gram_target):\n",
    "    return tf.reduce_mean(tf.square(gram_matrix(base)-gram_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e2ab8",
   "metadata": {},
   "source": [
    "<h2> Total Loss:</h2>\n",
    "\n",
    "We cannot add the content loss and style loss directly and use that as total loss. We need to apply the style over our content. Even from this statement we can understand that the ratio od them can not be equal. i mean we cannot give equal weights to them. We want to neural network to adjust some weight based on what target we want. So we take the weight as a linear sum with parameters α and β.\n",
    "\n",
    "<h3>Ltotal = αLcontent + βLstyle</h3><br>\n",
    "Thats it!!, this is all math we need for style transfer. We need to reduce the total loss and adjust Tstyle & T content to get a target image which has content of the content image and style of out original style image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4e3711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model,base,style_reference_image,content_weight,style_weight):\n",
    "    content_output=model(base)\n",
    "    style_output=model(style_reference_image)\n",
    "    \n",
    "    content_loss=0\n",
    "    style_loss=0\n",
    "    \n",
    "    for target_content, comb_content in zip(content_output, content_model(base)):\n",
    "        content_loss += get_content_loss(comb_content[0], target_content)\n",
    "    \n",
    "    for target_style, comb_style in zip(style_outputs, style_model(base_image)):\n",
    "        style_loss += get_style_loss(comb_style[0], target_style)\n",
    "        \n",
    "    content_loss=content_loss*content_weight/len(content_layers)\n",
    "    style_loss=style_loss*style_weight/len(style_layers)\n",
    "    \n",
    "    return total_loss,content_loss,style_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9c22821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(config, base_image, style_reference_image, content_weight, style_weight):\n",
    "    # Open a gradient tape to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute the total loss, content loss, and style loss using the provided model\n",
    "        loss, content_loss, style_loss = compute_loss(model, base_image, style_reference_image, content_weight, style_weight)\n",
    "\n",
    "    # Calculate the gradients of the total loss with respect to the base image\n",
    "    gradients = tape.gradient(loss, base_image)\n",
    "\n",
    "    # Return the computed gradients along with the individual losses\n",
    "    return gradients, loss, content_loss, style_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f715385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    # Load the image from the file path\n",
    "    img = keras_image.load_img(image_path, target_size=target_size)\n",
    "    \n",
    "    # Convert the PIL image to a NumPy array\n",
    "    img_array = keras_image.img_to_array(img)\n",
    "    \n",
    "    # Expand the dimensions to create a batch of size 1\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Preprocess the image using the VGG19 preprocess_input function\n",
    "    img_preprocessed = preprocess_input(img_array)\n",
    "    \n",
    "    # Convert the NumPy array back to a TensorFlow tensor\n",
    "    img_tensor = tf.convert_to_tensor(img_preprocessed)\n",
    "    \n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "855ac1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(content_path, style_path, num_iterations=1000, content_weight=1e3, style_weight=1e-2):\n",
    "    content_image = load_and_preprocess_image(content_path)\n",
    "    style_image = load_and_preprocess_image(style_path)\n",
    "\n",
    "    # Initialize the generated image with the content image\n",
    "    generated_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=0.5, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "    # Create a checkpoint to save the generated image during training\n",
    "    checkpoint = tf.train.Checkpoint(generated_image=generated_image)\n",
    "\n",
    "    # Create a custom configuration to use during training\n",
    "    config = {\n",
    "        'num_iterations': num_iterations,\n",
    "        'content_weight': content_weight,\n",
    "        'style_weight': style_weight\n",
    "    }\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        gradients, loss, content_loss, style_loss = compute_gradients(config, generated_image, style_image, content_weight, style_weight)\n",
    "        optimizer.apply_gradients([(gradients, generated_image)])\n",
    "        clipped_image = tf.clip_by_value(generated_image, 0.0, 255.0)\n",
    "        generated_image.assign(clipped_image)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Total Loss: {loss.numpy()}, Content Loss: {content_loss.numpy()}, Style Loss: {style_loss.numpy()}\")\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            # Save the generated image\n",
    "            checkpoint.save(file_prefix='./checkpoints/ckpt-')\n",
    "\n",
    "    return generated_image.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e88bd1f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [14,512] vs. [7,7,512] [Op:Sub] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m content_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofile.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m style_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonet.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m output_image \u001b[38;5;241m=\u001b[39m \u001b[43mrun_style_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(output_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 22\u001b[0m, in \u001b[0;36mrun_style_transfer\u001b[1;34m(content_path, style_path, num_iterations, content_weight, style_weight)\u001b[0m\n\u001b[0;32m     15\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m: num_iterations,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: content_weight,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: style_weight\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m---> 22\u001b[0m     gradients, loss, content_loss, style_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mapply_gradients([(gradients, generated_image)])\n\u001b[0;32m     24\u001b[0m     clipped_image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(generated_image, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m255.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 5\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[1;34m(config, base_image, style_reference_image, content_weight, style_weight)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradients\u001b[39m(config, base_image, style_reference_image, content_weight, style_weight):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Open a gradient tape to record operations for automatic differentiation\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# Compute the total loss, content loss, and style loss using the provided model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m         loss, content_loss, style_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_reference_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Calculate the gradients of the total loss with respect to the base image\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, base_image)\n",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(model, base, style_reference_image, content_weight, style_weight)\u001b[0m\n\u001b[0;32m      6\u001b[0m style_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_content, comb_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(content_output, content_model(base)):\n\u001b[1;32m----> 9\u001b[0m     content_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mget_content_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomb_content\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_style, comb_style \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(style_outputs, style_model(base_image)):\n\u001b[0;32m     12\u001b[0m     style_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m get_style_loss(comb_style[\u001b[38;5;241m0\u001b[39m], target_style)\n",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m, in \u001b[0;36mget_content_loss\u001b[1;34m(base, target)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_content_loss\u001b[39m(base,target):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(\u001b[43mbase\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtarget\u001b[49m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [14,512] vs. [7,7,512] [Op:Sub] name: "
     ]
    }
   ],
   "source": [
    "content_path = 'profile.jpg'\n",
    "style_path = 'monet.jpeg'\n",
    "output_image = run_style_transfer(content_path, style_path)\n",
    "\n",
    "# Display the result\n",
    "plt.imshow(output_image/255.0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
